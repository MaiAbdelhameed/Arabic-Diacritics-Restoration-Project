{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5RXIqzN_vcU"
      },
      "source": [
        "Preprocessing ideas:\n",
        "1) Removal of not arabic\n",
        "2) Removal and saving of diacritics\n",
        "3) Segementation\n",
        "4) Tokenization\n",
        "5) Lemmatization\n",
        "\n",
        "Feature Extraction ideas:\n",
        "1) POS\n",
        "2) Morphological quadruples\n",
        "3) Context\n",
        "4) Last character\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xxDR0bAP_vcY",
        "outputId": "f0eaae8b-e7c5-4196-e5e2-c6a8c33526ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from pyarabic) (1.16.0)\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.15\n"
          ]
        }
      ],
      "source": [
        "!pip install pyarabic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cOi78Pjl_vca",
        "outputId": "9e40a947-f57d-4b64-d86c-cb448105072e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_input(file_path) -> list:\n",
        "    with open(file_path, 'r') as file:\n",
        "    # Read the file line by line into a list\n",
        "        return file.readlines()"
      ],
      "metadata": {
        "id": "cW-SWmk-AO-t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KmzeJ9EE_vca"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pyarabic.araby as araby\n",
        "from pyarabic.araby import strip_tashkeel, is_arabicrange, strip_diacritics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xSazzkhi_vca"
      },
      "outputs": [],
      "source": [
        "def remove_diacritics_and_save(text):\n",
        "    stripped = araby.strip_diacritics(text)\n",
        "    diacritics_dic = [{i: char} for i,char in enumerate(text) if char not in stripped]\n",
        "    return stripped, diacritics_dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lbvjOn93_vcb"
      },
      "outputs": [],
      "source": [
        "def remove_non_arabic_char(text):\n",
        "    regex = re.compile('[^؀-ۿ ]')\n",
        "    result = regex.sub('', text)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenization(text):\n",
        "    return word_tokenize(text)\n"
      ],
      "metadata": {
        "id": "2BVu3aUz_3Z9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yQuaowT4_vcb",
        "outputId": "3ced6e0a-fc61-4929-8ca9-462217b3fab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-5eb96ded9db1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataset/train.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_non_arabic_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstripped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiacritics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_diacritics_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-c2aba27a1584>\u001b[0m in \u001b[0;36mread_input\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Read the file line by line into a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Dataset/train.txt'"
          ]
        }
      ],
      "source": [
        "train_dataset = read_input('/content/train.txt')\n",
        "text = train_dataset[0]\n",
        "result = remove_non_arabic_char(text)\n",
        "stripped, diacritics = remove_diacritics_and_save(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RAHOYhR_vcb",
        "outputId": "34dcab9e-5d90-4807-a957-024f7672e74d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "longest sentence is والله أسأل ، وبرسوله أتوسل ، أن ينفع به كما نفع بأصله ، وأن يغفر لمن نظر فيه بعين الإنصاف ، ودعا لمؤلفه بأن يدركه ربه جل وعلا بخفي الألطاف ، وبأن يمتعه بالنظر إلى وجهه ، ويمده بالإسعاف ، وحسبنا الله ونعم الوكيل ، ولا حول ولا قوة( 29 / 477 ). قوله : ( لاعن امرأته ) قال في الفتح : اللعان مأخوذ من اللعن ; لأن الملاعن يقول في الخامسة : لعنة الله عليه إن كان من الكاذبين , واختير لفظ اللعن دون الغضب في التسمية لأنه قول الرجل وهو الذي بدئ به في الآية , وهو أيضا يبدأ به . وقيل : سمي لعانا لأن اللعن : الطرد والإبعاد , وهو مشترك بينهما . وإنما خصت المرأة بلفظ الغضب لعظم الذنب بالنسبة إليها . ثم قال : وأجمعوا على أن اللعان مشروع , وعلى أنه لا يجوز مع عدم التحقق . واختلف في وجوبه على الزوج . وظاهر أحاديث الباب أن اللعان إنما يشرع بين الزوجين , وكذلك قوله تعالى : { والذين يرمون أزواجهم } الآية , فلو قال أجنبي لأجنبية : يا زانية وجب عليه حد القذف . قوله : ( ففرق رسول الله صلى الله عليه وسلم بينهما ) استدل به من قال إن الفرقة بين المتلاعنين لا تقع بنفس اللعان حتى يوقعها الحاكم وأجاب من قال : إن الفرقة تقع بنفس اللعان أن ذلك بيان حكم لا إيقاع فرقة . واحتجوا بما وقع منه صلى الله عليه وسلم في رواية بلفظ : { لا سبيل لك عليها } . وتعقب بأن الذي وقع جواب لسؤال الرجل عن ماله الذي أخذته منه . وأجيب بأن العبرة بعموم اللفظ , وهو نكرة في سياق النفي فيشمل المال والبدن ويقتضي نفي تسلطه عليها بوجه من الوجوه . ووقع في حديث لأبي داود عن ابن عباس { وقضى أن ليس عليه قوت ولا سكنى من أجل أنهما يفترقان بغير طلاق ولا متوفى عنها } , وهو ظاهر في أن الفرقة وقعت بينهما بنفس اللعان , وسيأتي تمام الكلام في الفرقة في الباب الذي بعد هذا : قوله : ( وألحق الولد بالمرأة ) قال الدارقطني : تفرد مالك بهذه الزيادة . وقال ابن عبد البر : ذكروا أن مالكا تفرد بهذه اللفظة , وقد جاءت من أوجه أخر , وقد جاءت في حديث سهل بن سعد عند أبي داود بلفظ : { فكان الولد ينسب إلى أمه } ومن رواية أخرى { وكان الولد يدعى إلى أمه } ومعنى قوله { ألحق الولد بأمه } : أي صيره لها وحدها ونفاه عن الزوج فلا توارث بينهما , وأما الأم فترث منه ما فرض الله لها وقد وقع في رواية من حديث سهل بن سعد بلفظ : { وكان ابنها يدعى لأمه } ثم جرت السنة في ميراثها أنها ترثه ويرث منها ما فرض الله لهما . وقيل : معنى إلحاقه بأمه أنه صيرها له أبا وأما , فترث جميع ماله إذا لم يكن له وارث آخر من ولد ونحوه , وهو قول ابن مسعود وواثلة وطائفة ورواية عن أحمد , وروي أيضا عن القاسم , وقيل : إن عصبة أمه تصير عصبة له , وهو قول علي وابن عمر وهو المشهور عن أحمد , وبه قالت الهادوية . وقيل : ترثه أمه وأخته منها بالفرض والرد , وهو قول أبي عبيد ومحمد بن الحسن ورواية عن أحمد قال : فإن لم يرثه ذو فرض بحال فعصبته عصبة أمه . واستدل بحديث ابن عمر المذكور على مشروعية اللعان لنفي الولد , وعن أحمد ينتفي الولد بمجرد اللعان وإن لم يتعرض الرجل لذكره في اللعان . قال الحافظ : وفيه نظر لأنه لو استلحقه لحقه , وإنما يؤثر اللعان دفع حد القذف عنه وثبوت زنى المرأة وقال الشافعي : إن نفي الولد في الملاعنة انتفى وإن لم يتعرض له , فله أن يعيد اللعان لانتفائه ولا إعادة على المرأة , وإن أمكنه الرفع إلى الحاكم فأخر بغير عذر حتى ولدت لم يكن له أن ينفيه . كما في الشفعة , واستدل به أيضا على أنه لا يشترط في نفي الولد التصريح بأنها ولدته من زنى ولا بأنه استبرأها بحيضة . وعن المالكية يشترط ذلك . قوله : ( أرأيت لو وجد أحدنا ) أي أخبرني عن حكم من وقع له ذلك . قوله : ( على فاحشة ) اختلف العلماء فيمن وجد مع امرأته رجلا وتحقق وجود الفاحشة منهما فقتله هل يقتل به أم لا ؟ فمنع الجمهور الإقدام وقالوا : يقتص منه إلا أن يأتي ببينة الزنى أو يعترف المقتول بذلك بشرط أن يكون محصنا . وقيل : بل يقتل به لأنه ليس له أن يقيم الحد بغير إذن الإمام . وقال بعض السلف : لا يقتل أصلا ويعذر فيما فعله إذا ظهرت أمارات صدقه , وشرط أحمد وإسحاق ومن تبعهما أن يأتي بشاهدين أنه قتله بسبب ذلك . ووافقهم ابن القاسم وابن حبيب من المالكية لكن زاد أن يكون المقتول قد أحصن وعند الهادوية أنه يجوز للرجل أن يقتل من وجده مع زوجته وأمته وولده حال الفعل , وأما بعده فيقاد به إن كان بكرا . قوله : ( ووعظه وذكره ) فيه دليل على أنه يشرع للإمام موعظة المتلاعنين قبل اللعان تحذيرا لهما منه وتخويفا لهما من الوقوع في المعصية . قوله : ( فبدأ بالرجل ) فيه دليل على أنه يبدأ الإمام في اللعان بالرجل . وقد حكى الإمام المهدي في البحر الإجماع . أن السنة تقديم الزوج . واختلف في الوجوب ; فذهب الشافعي ومن تبعه وأشهب من المالكية ورجحه ابن العربي إلى أنه واجب وهو قول المؤيد بالله وأبي طالب وأبي العباس والإمام يحيى . وذهب الحنفية ومالك وابن القاسم إلى أنه لو وقع الابتداء بالمرأة صح واعتد به ; واحتجوا بأن الله تعالى عطف في القرآن بالواو وهو لا يقتضي الترتيب ; واحتج الأولون أيضا بأن اللعان يشرع لدفع الحد عن الرجل , ويؤيده قوله صلى الله عليه وسلم لهلال : \" البينة وإلا حد في ظهرك \" وسيأتي , فلو بدأ بالمرأة لكان دفعا لأمر لم يثبت . قوله : ( بين أخوي بني عجلان ) بفتح العين المهملة وسكون الجيم وهو ابن حارثة بن ضبيعة من بني بكر بن عمرو , والمراد بقوله \" أخوي \" الرجل وامرأته , واسم الرجل عويمر كما في الرواية المذكورة , واسم المرأة خولة بنت عاصم بن عدي العجلاني قاله ابن منده في كتاب الصحابة وأبو نعيم وحكى القرطبي عن مقاتل بن سليمان أنها خولة بنت قيس , وذكر ابن مردويه أنها بنت أخي عاصم المذكور , والرجل الذي رمى عويمر امرأته به هو شريك بن سحماء ابن عم عويمر , وفي صحيح مسلم من حديث أنس : { أن هلال بن أمية قذف امرأته بشريك بن سحماء وكان أخا البراء بن مالك لأمه } وسيأتي , وكان أول رجل لاعن في الإسلام . قال النووي في شرح مسلم : السبب في نزول آية اللعان قصة عويمر العجلاني واستدل على ذلك بقوله صلى الله عليه وسلم له : { قد أنزل الله فيك وفي صاحبتك قرآنا } وقال الجمهور . : السبب قصة هلال بن أمية لما تقدم من أنه كان أول رجل لاعن في الإسلام . وقد حكى أيضا الماوردي عن الأكثر أن قصة هلال أسبق من قصة عويمر . وقال الخطيب والنووي وتبعهما الحافظ : يحتمل أن يكون هلال سأله أولا ثم سأل عويمرا فنزلت في شأنهما معا وقال ابن الصباغ في الشامل : قصة هلال بن أمية نزلت فيها الآية . وأما قوله صلى الله عليه وسلم لعويمر : \" إن الله قد أنزل فيك وفي صاحبتك \" فمعناه ما نزل في قصة هلال لأن ذلك حكم عام لجميع الناس . واختلف في الوقت الذي وقع فيه اللعان ; فجزم الطبري وأبو حاتم وابن حبان أنه كان في شهر شعبان سنة تسع , وقيل : كان في السنة التي توفي فيها رسول الله صلى الله عليه وسلم , لما وقع في البخاري عن سهل بن سعد أنه شهد قصة المتلاعنين وهو ابن خمس عشرة سنة , وقد ثبت عنه أنه قال توفي رسول الله صلى الله عليه وسلم وأنا ابن خمس عشرة سنة . وقيل : كانت القصة في سنة عشر , ووفاته صلى الله عليه وسلم في سنة إحدى عشرة . قوله : ( فطلقها ثلاثا ) وفي رواية أنه قال : فهي الطلاق فهي الطلاق فهي الطلاق وقد استدل بذلك من قال : إن الفرقة بين المتلاعنين تتوقف على تطليق الرجل كما تقدم نقله عن عثمان البتي . وأجيب بما في حديث سهل نفسه من تفريقه صلى الله عليه وسلم بينهما وبما في حديث ابن عمر كما ذكر ذلك المصنف فإن ظاهرهما أن الفرقة وقعت بتفريق النبي صلى الله عليه وسلم وإنما طلقها عويمر لظنه أن اللعان لا يحرمها عليه فأراد تحريمها بالطلاق فقال : طالق ثلاثا , فقال له النبي صلى الله عليه وسلم لا سبيل لك عليها أي لا ملك لك عليها فلا يقع طلاق . قال الحافظ : وقد توهم أن قوله : \" لا سبيل لك عليها \" وقع منه صلى الله عليه وسلم عقب قول الملاعن هي طالق , وأنه موجود كذلك في حديث سهل , وإنما وقع في حديث ابن عمر عقب قوله : { الله يعلم أن أحدكما كاذب , لا سبيل لك عليها } انتهى . وقد قدمنا ما جاء في طلاق البتة الجواب عن الاستدلال بهذا الحديث على أن الطلاق المتتابع يقع : قوله : ( فكانت سنة المتلاعنين ) زاد أبو داود عن القعنبي عن مالك \" فكانت تلك \" وهي إشارة إلى الفرقة . وفي الرواية الأخرى المذكورة ذاكم التفريق بين كل متلاعنين وقال مسلم : إن قوله : وكان فراقه إياها سنة بين المتلاعنين مدرج . وكذا ذكر الدارقطني في غريب مالك اختلاف الرواة على ابن شهاب ثم على مالك في تعيين من قال : \" فكان فراقهما سنة \" هل هو من قول سهل , أو من قول ابن شهاب ؟ وذكر ذلك الشافعي وأشار إلى أن نسبته إلى ابن شهاب لا تمنع نسبته إلى سهل ويؤيد ذلك ما وقع في رواية لأبي داود عن سهل قال : { فطلقها ثلاث تطليقات عند رسول الله صلى الله عليه وسلم , فأنفذه رسول الله صلى الله عليه وسلم , وكان ما صنع عند رسول الله صلى الله عليه وسلم سنة } وسيأتي قريبا . وفي نسخة الصاغاني قال أبو عبد الله : قوله : \" ذلك تفريق بين المتلاعنين \" من قول الزهري وليس من الحديث\n",
            " with length = 7543\n",
            "shortest sentence is ذلك\n",
            " with length = 4\n"
          ]
        }
      ],
      "source": [
        "longest_sentence = \"\"\n",
        "shortest_sentence = \"\"\n",
        "max_len = 0\n",
        "min_len = 1000000000\n",
        "for t in train_dataset:\n",
        "    t,_ = remove_diacritics_and_save(t)\n",
        "    if (len(t) > max_len):\n",
        "        max_len = len(t)\n",
        "        longest_sentence= t\n",
        "\n",
        "    if (len(t) < min_len):\n",
        "        min_len = len(t)\n",
        "        shortest_sentence = t\n",
        "\n",
        "\n",
        "print(f\"longest sentence is {longest_sentence} with length = {max_len}\")\n",
        "print(f\"shortest sentence is {shortest_sentence} with length = {min_len}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOSvJnTr_vcc",
        "outputId": "f6f99f76-7d47-45e9-8998-8ba890091c20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1885.75\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrUrZJLF_vcc",
        "outputId": "fe4308f4-f088-4a24-fa85-18ba6bd90a74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: qalsadi in c:\\users\\mai\\anaconda3\\lib\\site-packages (0.5)\n",
            "Requirement already satisfied: pyarabic>=0.6.7 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from qalsadi) (0.6.15)\n",
            "Requirement already satisfied: tashaphyne>=0.3.4.1 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from qalsadi) (0.3.6)\n",
            "Requirement already satisfied: Arabic-Stopwords>=0.4.2 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from qalsadi) (0.4.3)\n",
            "Requirement already satisfied: naftawayh>=0.3 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from qalsadi) (0.4)\n",
            "Requirement already satisfied: mysam-tagmanager>=0.3.3 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from qalsadi) (0.4)\n",
            "Requirement already satisfied: arramooz-pysqlite>=0.4.2 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from qalsadi) (0.4.2)\n",
            "Requirement already satisfied: libqutrub>=1.2.3 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from qalsadi) (1.2.4.1)\n",
            "Requirement already satisfied: alyahmor>=0.2 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from qalsadi) (0.2)\n",
            "Requirement already satisfied: pickledb>=0.9.2 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from qalsadi) (0.9.2)\n",
            "Requirement already satisfied: codernitydb3 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from qalsadi) (0.6.0)\n",
            "Requirement already satisfied: six>=1.14.0 in c:\\users\\mai\\appdata\\roaming\\python\\python39\\site-packages (from pyarabic>=0.6.7->qalsadi) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install qalsadi\n",
        "import qalsadi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiV2D7Z9_vcd"
      },
      "outputs": [],
      "source": [
        "import qalsadi.lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSDzHCta_vcd",
        "outputId": "240ba55a-480f-4915-baee-d09ed656892f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['عل']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
        "lemmas = lemmer.lemmatize_text('فعل')\n",
        "lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RpM9l9a_vcd",
        "outputId": "63dd8e8f-4f18-4e64-c201-0991d043068a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://test.pypi.org/simple/\n",
            "Requirement already satisfied: FarasaPy3 in c:\\users\\mai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install --index-url https://test.pypi.org/simple/ --no-deps FarasaPy3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xN88sAb_vcd",
        "outputId": "6726c3d7-99be-4d0b-9c9e-099ca0e9ce52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement FarasaPy3 (from versions: none)\n",
            "ERROR: No matching distribution found for FarasaPy3\n"
          ]
        }
      ],
      "source": [
        "%pip install FarasaPy3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePGY_e_i_vce",
        "outputId": "723262cf-f671-446f-e119-526ec3213387"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'FarasaPy3'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mFarasaPy3\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mFarasaPy3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FarasaPy3\n\u001b[0;32m      3\u001b[0m farasaApi \u001b[38;5;241m=\u001b[39m FarasaPy3()\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'FarasaPy3'"
          ]
        }
      ],
      "source": [
        "import FarasaPy3\n",
        "from FarasaPy3.api import FarasaPy3\n",
        "farasaApi = FarasaPy3()\n",
        "result = farasaApi.Segmentation(text)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHToL2tq_vce"
      },
      "source": [
        "#Features:\n",
        "1) last char identity\n",
        "2) raw word\n",
        "3) context\n",
        "4) context class\n",
        "5) POS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wfaDLjwd_vce",
        "outputId": "e750727e-43a0-4373-a54a-723837caa0b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting farasa\n",
            "  Downloading Farasa-0.0.1-py2.py3-none-any.whl (12.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: farasa\n",
            "Successfully installed farasa-0.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install farasa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "uaBAQtzR_vce",
        "outputId": "66f9236a-b0fe-4b1d-bae3-e306f268d953",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-6a1dcff39ddf>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# import FarasaPOSTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"هذا مثال على استخدام تسميات أجزاء الكلام في اللغة العربية.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfarasa_pos_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfarasa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpos_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfarasa_pos_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'farasa' has no attribute 'pos_tag'"
          ]
        }
      ],
      "source": [
        "import farasa\n",
        "# import FarasaPOSTagger\n",
        "text = \"هذا مثال على استخدام تسميات أجزاء الكلام في اللغة العربية.\"\n",
        "farasa_pos_tagger = farasa.pos_tag(text)\n",
        "\n",
        "pos_tags = farasa_pos_tagger.tag(text)\n",
        "print(pos_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_SGN5gmG_vcf",
        "outputId": "c54d6f6c-36cd-4e22-e458-db79ad943047",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arabicnlp\n",
            "  Downloading arabicnlp-0.1.7-py3-none-any.whl (14.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.4/14.4 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from arabicnlp) (2.14.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from arabicnlp) (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (1.59.3)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->arabicnlp) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->arabicnlp) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->arabicnlp) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->arabicnlp) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->arabicnlp) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->arabicnlp) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->arabicnlp) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->arabicnlp) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->arabicnlp) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->arabicnlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->arabicnlp) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->arabicnlp) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->arabicnlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->arabicnlp) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->arabicnlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->arabicnlp) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow->arabicnlp) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->arabicnlp) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->arabicnlp) (3.2.2)\n",
            "Installing collected packages: arabicnlp\n",
            "Successfully installed arabicnlp-0.1.7\n"
          ]
        }
      ],
      "source": [
        "!pip install arabicnlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arabicnlp\n",
        "arabicnlp.arabic_pos_tagger(text)"
      ],
      "metadata": {
        "id": "uKVSF6iABU0K",
        "outputId": "630536e6-c96a-4039-a352-749a4291046a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-4d267774f6f0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0marabicnlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0marabicnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marabic_pos_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/arabicnlp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.1.3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/arabicnlp/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArabicStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpos_tagger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcorrection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspell_checker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/arabicnlp/pos_tagger.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'ignore_accuracy'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_ignore_class_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mglobal\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWORDINDEX_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TwOh6A_mBUnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8nq78ae_vcf",
        "outputId": "65cee57b-c68b-4123-f4ea-7ac75b4ade42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('يذهب', 'NN'), ('علي', 'NN')]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting arabicnlp\n",
            "  Using cached arabicnlp-0.1.7-py3-none-any.whl (14.4 MB)\n",
            "Collecting keras\n",
            "  Using cached keras-3.0.1-py3-none-any.whl (999 kB)\n",
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.15.0-cp39-cp39-win_amd64.whl (2.1 kB)\n",
            "Requirement already satisfied: h5py in c:\\users\\mai\\anaconda3\\lib\\site-packages (from keras->arabicnlp) (3.6.0)\n",
            "Collecting dm-tree\n",
            "  Using cached dm_tree-0.1.8-cp39-cp39-win_amd64.whl (101 kB)\n",
            "Collecting absl-py\n",
            "  Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\mai\\anaconda3\\lib\\site-packages (from keras->arabicnlp) (1.26.2)\n",
            "Collecting rich\n",
            "  Using cached rich-13.7.0-py3-none-any.whl (240 kB)\n",
            "Collecting namex\n",
            "  Using cached namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mai\\appdata\\roaming\\python\\python39\\site-packages (from rich->keras->arabicnlp) (2.13.0)\n",
            "Collecting markdown-it-py>=2.2.0\n",
            "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Collecting mdurl~=0.1\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting tensorflow-intel==2.15.0\n",
            "  Downloading tensorflow_intel-2.15.0-cp39-cp39-win_amd64.whl (300.8 MB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
            "    yield\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 519, in read\n",
            "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 62, in read\n",
            "    data = self.__fp.read(amt)\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\http\\client.py\", line 463, in read\n",
            "    n = self.readinto(b)\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\http\\client.py\", line 507, in readinto\n",
            "    n = self.fp.readinto(b)\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\socket.py\", line 704, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\ssl.py\", line 1241, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\ssl.py\", line 1099, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 173, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 203, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 315, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 94, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 472, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 366, in resolve\n",
            "    failure_causes = self._attempt_to_pin_criterion(name)\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 212, in _attempt_to_pin_criterion\n",
            "    criteria = self._get_updated_criteria(candidate)\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 203, in _get_updated_criteria\n",
            "    self._add_to_criteria(criteria, requirement, parent=candidate)\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 172, in _add_to_criteria\n",
            "    if not criterion.candidates:\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 151, in __bool__\n",
            "    return bool(self._sequence)\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 140, in __bool__\n",
            "    return any(self)\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 128, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 32, in _iter_built\n",
            "    candidate = func()\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 204, in _make_candidate_from_link\n",
            "    self._link_candidate_cache[link] = LinkCandidate(\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 295, in __init__\n",
            "    super().__init__(\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 156, in __init__\n",
            "    self.dist = self._prepare()\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 227, in _prepare\n",
            "    dist = self._prepare_distribution()\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 305, in _prepare_distribution\n",
            "    return self._factory.preparer.prepare_linked_requirement(\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 508, in prepare_linked_requirement\n",
            "    return self._prepare_linked_requirement(req, parallel_builds)\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 550, in _prepare_linked_requirement\n",
            "    local_file = unpack_url(\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 239, in unpack_url\n",
            "    file = get_http_url(\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 102, in get_http_url\n",
            "    from_path, content_type = download(link, temp_dir.path)\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 145, in __call__\n",
            "    for chunk in chunks:\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 144, in iter\n",
            "    for x in it:\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 63, in response_chunks\n",
            "    for chunk in response.raw.stream(\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 576, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 541, in read\n",
            "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\contextlib.py\", line 137, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"C:\\Users\\Mai\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 455, in _error_catcher\n",
            "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
            "pip._vendor.urllib3.exceptions.ProtocolError: (\"Connection broken: ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)\", ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n"
          ]
        }
      ],
      "source": [
        "sentence = \"يذهب علي\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "print(tagged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_oozkls2_vcf",
        "outputId": "ebe2709a-788b-4c54-f17b-00de25b8a133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a925928058cc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0marabicnlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/arabicnlp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.1.3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/arabicnlp/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArabicStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpos_tagger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcorrection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspell_checker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/arabicnlp/pos_tagger.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'ignore_accuracy'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_ignore_class_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mglobal\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWORDINDEX_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'"
          ]
        }
      ],
      "source": [
        "import arabicnlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKgwDMzY_vcg"
      },
      "outputs": [],
      "source": [
        "# import json\n",
        "# import requests\n",
        "# url = 'https://farasa.qcri.org/webapi/pos/'\n",
        "# text = 'يُشار إلى أن اللغة العربية'\n",
        "# api_key = \"#####################\"\n",
        "# payload = {'text': text, 'api_key': api_key}\n",
        "# data = requests.post(url, data=payload)\n",
        "# result = json.loads(data.text)\n",
        "# print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "VlpOjaSQ_vcg",
        "outputId": "9e8ae099-a6c7-416d-8c87-475e3fb75f24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanfordnlp\n",
            "  Downloading stanfordnlp-0.2.0-py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (1.23.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (2.31.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (4.66.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->stanfordnlp) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->stanfordnlp) (1.3.0)\n",
            "Installing collected packages: stanfordnlp\n",
            "Successfully installed stanfordnlp-0.2.0\n"
          ]
        }
      ],
      "source": [
        "%pip install stanfordnlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TytkN9AE_vcg",
        "outputId": "1d5180d4-8ff2-4917-a4b0-45b5606653ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/ar_padt_models/ar_padt_tokenizer.pt', 'lang': 'ar', 'shorthand': 'ar_padt', 'mode': 'predict'}\n",
            "Cannot load model from /root/stanfordnlp_resources/ar_padt_models/ar_padt_tokenizer.pt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/stanfordnlp/models/tokenize/trainer.py\", line 82, in load\n",
            "    checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 986, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 435, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 416, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/root/stanfordnlp_resources/ar_padt_models/ar_padt_tokenizer.pt'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-28-9061af932724>\", line 8, in <cell line: 8>\n",
            "    arabic_pos_tagger = stanfordnlp.Pipeline(processors='tokenize,pos', pos_model_path=arabic_pos_model_path, pos_jar_path=arabic_pos_tagger_path, lang='ar')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/stanfordnlp/pipeline/core.py\", line 121, in __init__\n",
            "    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/stanfordnlp/pipeline/processor.py\", line 102, in __init__\n",
            "    self._set_up_model(config, use_gpu)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/stanfordnlp/pipeline/tokenize_processor.py\", line 31, in _set_up_model\n",
            "    self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/stanfordnlp/models/tokenize/trainer.py\", line 16, in __init__\n",
            "    self.load(model_file)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/stanfordnlp/models/tokenize/trainer.py\", line 85, in load\n",
            "    sys.exit(1)\n",
            "SystemExit: 1\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "AttributeError: 'tuple' object has no attribute 'tb_frame'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stanfordnlp/models/tokenize/trainer.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/stanfordnlp_resources/ar_padt_models/ar_padt_tokenizer.pt'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-9061af932724>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Initialize the Arabic POS Tagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0marabic_pos_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanfordnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tokenize,pos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_model_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marabic_pos_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_jar_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marabic_pos_tagger_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stanfordnlp/pipeline/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, processors, lang, models_dir, treebank, use_gpu, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;31m# try to build processor, throw an exception if there is a requirements issue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n\u001b[0m\u001b[1;32m    122\u001b[0m                                                                                           \u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stanfordnlp/pipeline/processor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, pipeline, use_gpu)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_up_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;31m# run set up process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stanfordnlp/pipeline/tokenize_processor.py\u001b[0m in \u001b[0;36m_set_up_model\u001b[0;34m(self, config, use_gpu)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stanfordnlp/models/tokenize/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, vocab, model_file, use_cuda)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# load everything from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stanfordnlp/models/tokenize/trainer.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot load model from {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: 1",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2091\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2092\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2093\u001b[0m                                                                      value))\n\u001b[1;32m   2094\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ],
      "source": [
        "import stanfordnlp\n",
        "\n",
        "# Set the path to the Arabic POS Tagger JAR file and models\n",
        "arabic_pos_tagger_path = '/path/to/stanford-postagger-3.9.2.jar'\n",
        "arabic_pos_model_path = '/path/to/arabic-ud.tagger'\n",
        "\n",
        "# Initialize the Arabic POS Tagger\n",
        "arabic_pos_tagger = stanfordnlp.Pipeline(processors='tokenize,pos', pos_model_path=arabic_pos_model_path, pos_jar_path=arabic_pos_tagger_path, lang='ar')\n",
        "\n",
        "# Example text\n",
        "text = \"هذا مثال على استخدام تسميات أجزاء الكلام في اللغة العربية.\"\n",
        "\n",
        "# Process the text and obtain POS tags\n",
        "doc = arabic_pos_tagger(text)\n",
        "\n",
        "# Access POS tags for each word in the text\n",
        "for sentence in doc.sentences:\n",
        "    for word in sentence.words:\n",
        "        print(f'{word.text}: {word.pos}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKcInl44_vcg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}