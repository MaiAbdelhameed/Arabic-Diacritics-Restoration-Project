{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "PtGAljf7Rspl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for PyArabic\n",
        "https://github.com/linuxscout/pyarabic/blob/master/doc/features.md"
      ],
      "metadata": {
        "id": "5RhdjPLXWAgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarabic\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import pyarabic.araby as araby\n",
        "from pyarabic.araby import is_arabicrange, strip_diacritics\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import tensorflow as tf\n",
        "from gensim.models import KeyedVectors\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import numpy\n",
        "from numpy import array\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "2GqIYMMkRq_A",
        "outputId": "d771ec0a-5a60-4c58-aaae-00d405162548",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.10/dist-packages (0.6.15)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from pyarabic) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inputs\n"
      ],
      "metadata": {
        "id": "jZudPvGORiVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " def read_input(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        content =  file.readlines()\n",
        "    return content"
      ],
      "metadata": {
        "id": "iCo5QAagRh9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = read_input('train.txt')\n",
        "val_dataset = read_input('val.txt')"
      ],
      "metadata": {
        "id": "kLWbN-PPS4R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "O6GdMtxjUEJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_diacritics_and_save(text):\n",
        "    stripped = araby.strip_diacritics(text)\n",
        "    diacritics_dic = [{i: char} for i,char in enumerate(text) if char not in stripped]\n",
        "    return stripped, diacritics_dic"
      ],
      "metadata": {
        "id": "Fg6QbU8tUGjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_non_arabic_char(text):\n",
        "    regex = re.compile('[^؀-ۿ ,.،؛]')\n",
        "    result = regex.sub('', text)\n",
        "    return result"
      ],
      "metadata": {
        "id": "CexGSm8hULu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitting(text):\n",
        "    return text.split()"
      ],
      "metadata": {
        "id": "VsIjzvLY_fqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def segmenter(text):\n",
        "    pattern = r\"[.؟!؛،,]+|,\"\n",
        "    sentences = re.split(pattern, text)\n",
        "    # Remove empty strings from the list\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "C4IrUfUlDMaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segmented = segmenter(train_dataset[5])\n",
        "segmented\n",
        "\n",
        "for s in segmented:\n",
        "    print(word_tokenize(s))"
      ],
      "metadata": {
        "id": "3uMPHAXRSWKJ",
        "outputId": "540a9ac9-bd3f-4fe9-9c55-0e4a251e42f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['وَقَوْلُهُ', '(', 'لَزِمَتْهُ', 'لِمَا', 'قُلْنَا', ')', 'يُرِيدُ', 'قَوْلَهُ']\n",
            "['لِأَنَّ', 'الْجَمْعَ', 'بَيْنَهُمَا', 'مَشْرُوعٌ', 'فِي', 'حَقِّ', 'الْآفَاقِيِّ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arabic_only = remove_non_arabic_char(train_dataset[2])\n",
        "print('without non arabic char: ', arabic_only)\n",
        "splitted = splitting(arabic_only)\n",
        "print(splitted)\n",
        "\n",
        "stripped_words = []\n",
        "dict_of_diacritics = []\n",
        "for s in splitted:\n",
        "    stripped, dict_diacritics = remove_diacritics_and_save(s)\n",
        "    segmented = segmenter(stripped)\n",
        "    stripped_words.append(segmented)\n",
        "    dict_of_diacritics.append(dict_diacritics)\n",
        "print(\"stripped words = \", stripped_words)\n",
        "print(\"dict of diacritics = \", dict_of_diacritics)"
      ],
      "metadata": {
        "id": "MTFbrfYq-3NR",
        "outputId": "52d453dd-2aab-4957-f237-2d61af18583e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "without non arabic char:   قَوْلُهُ لِعَدَمِ مَا تَتَعَلَّقُ إلَخْ  أَيْ الْوَصِيَّةُ  قَوْلُهُ مَا مَرَّ  أَيْ قُبَيْلَ قَوْلِ الْمَتْنِ لَغَتْ وَلَوْ اقْتَصَرَ عَلَى أَوْصَيْت لَهُ بِشَاةٍ أَوْ أَعْطُوهُ شَاةً وَلَا غَنَمَ لَهُ عِنْدَ الْمَوْتِ هَلْ تَبْطُلُ الْوَصِيَّةُ أَوْ يُشْتَرَى لَهُ شَاةٌ وَيُؤْخَذُ مِنْ قَوْلِهِ الْآتِي كَمَا لَوْ لَمْ يَقُلْ مِنْ مَالِي وَلَا مِنْ غَنَمِي أَنَّهَا لَا تَبْطُلُ ، وَعِبَارَةُ الْكَنْزِ وَلَوْ لَمْ يَقُلْ مِنْ مَالِي وَلَا مِنْ غَنَمِي لَمْ يَتَعَيَّنْ غَنَمُهُ إنْ كَانَتْ انْتَهَتْ ا ه سم  قَوْلُهُ فَيُعْطَى وَاحِدَةً مِنْهَا إلَخْ  كَمَا لَوْ كَانَتْ مَوْجُودَةً عِنْدَ الْوَصِيَّةِ وَالْمَوْتِ ، وَلَا يَجُوزُ أَنْ يُعْطَى وَاحِدَةً مِنْ غَيْرِ غَنَمِهِ فِي الصُّورَتَيْنِ وَإِنْ تَرَاضَيَا ؛ لِأَنَّهُ صُلْحٌ عَلَى مَجْهُولٍ مُغْنِي وَنِهَايَةٌ قَالَ ع ش قَوْلُهُ وَاحِدَةً مِنْهَا أَيْ كَامِلَةً ، وَلَا يَجُوزُ أَنْ يُعْطَى نِصْفَيْنِ مِنْ شَاتَيْنِ ؛ لِأَنَّهُ لَا يُسَمَّى شَاةً وَقَوْلُهُ وَلَا يَجُوزُ أَنْ يُعْطَى وَاحِدَةً مِنْ غَيْرِ غَنَمِهِ وَيَنْبَغِي أَنْ يُقَالَ مِثْلُ ذَلِكَ فِي الْأَرِقَّاءِ ا ه .\n",
            "['قَوْلُهُ', 'لِعَدَمِ', 'مَا', 'تَتَعَلَّقُ', 'إلَخْ', 'أَيْ', 'الْوَصِيَّةُ', 'قَوْلُهُ', 'مَا', 'مَرَّ', 'أَيْ', 'قُبَيْلَ', 'قَوْلِ', 'الْمَتْنِ', 'لَغَتْ', 'وَلَوْ', 'اقْتَصَرَ', 'عَلَى', 'أَوْصَيْت', 'لَهُ', 'بِشَاةٍ', 'أَوْ', 'أَعْطُوهُ', 'شَاةً', 'وَلَا', 'غَنَمَ', 'لَهُ', 'عِنْدَ', 'الْمَوْتِ', 'هَلْ', 'تَبْطُلُ', 'الْوَصِيَّةُ', 'أَوْ', 'يُشْتَرَى', 'لَهُ', 'شَاةٌ', 'وَيُؤْخَذُ', 'مِنْ', 'قَوْلِهِ', 'الْآتِي', 'كَمَا', 'لَوْ', 'لَمْ', 'يَقُلْ', 'مِنْ', 'مَالِي', 'وَلَا', 'مِنْ', 'غَنَمِي', 'أَنَّهَا', 'لَا', 'تَبْطُلُ', '،', 'وَعِبَارَةُ', 'الْكَنْزِ', 'وَلَوْ', 'لَمْ', 'يَقُلْ', 'مِنْ', 'مَالِي', 'وَلَا', 'مِنْ', 'غَنَمِي', 'لَمْ', 'يَتَعَيَّنْ', 'غَنَمُهُ', 'إنْ', 'كَانَتْ', 'انْتَهَتْ', 'ا', 'ه', 'سم', 'قَوْلُهُ', 'فَيُعْطَى', 'وَاحِدَةً', 'مِنْهَا', 'إلَخْ', 'كَمَا', 'لَوْ', 'كَانَتْ', 'مَوْجُودَةً', 'عِنْدَ', 'الْوَصِيَّةِ', 'وَالْمَوْتِ', '،', 'وَلَا', 'يَجُوزُ', 'أَنْ', 'يُعْطَى', 'وَاحِدَةً', 'مِنْ', 'غَيْرِ', 'غَنَمِهِ', 'فِي', 'الصُّورَتَيْنِ', 'وَإِنْ', 'تَرَاضَيَا', '؛', 'لِأَنَّهُ', 'صُلْحٌ', 'عَلَى', 'مَجْهُولٍ', 'مُغْنِي', 'وَنِهَايَةٌ', 'قَالَ', 'ع', 'ش', 'قَوْلُهُ', 'وَاحِدَةً', 'مِنْهَا', 'أَيْ', 'كَامِلَةً', '،', 'وَلَا', 'يَجُوزُ', 'أَنْ', 'يُعْطَى', 'نِصْفَيْنِ', 'مِنْ', 'شَاتَيْنِ', '؛', 'لِأَنَّهُ', 'لَا', 'يُسَمَّى', 'شَاةً', 'وَقَوْلُهُ', 'وَلَا', 'يَجُوزُ', 'أَنْ', 'يُعْطَى', 'وَاحِدَةً', 'مِنْ', 'غَيْرِ', 'غَنَمِهِ', 'وَيَنْبَغِي', 'أَنْ', 'يُقَالَ', 'مِثْلُ', 'ذَلِكَ', 'فِي', 'الْأَرِقَّاءِ', 'ا', 'ه', '.']\n",
            "stripped words =  [['قوله'], ['لعدم'], ['ما'], ['تتعلق'], ['إلخ'], ['أي'], ['الوصية'], ['قوله'], ['ما'], ['مر'], ['أي'], ['قبيل'], ['قول'], ['المتن'], ['لغت'], ['ولو'], ['اقتصر'], ['على'], ['أوصيت'], ['له'], ['بشاة'], ['أو'], ['أعطوه'], ['شاة'], ['ولا'], ['غنم'], ['له'], ['عند'], ['الموت'], ['هل'], ['تبطل'], ['الوصية'], ['أو'], ['يشترى'], ['له'], ['شاة'], ['ويؤخذ'], ['من'], ['قوله'], ['الآتي'], ['كما'], ['لو'], ['لم'], ['يقل'], ['من'], ['مالي'], ['ولا'], ['من'], ['غنمي'], ['أنها'], ['لا'], ['تبطل'], [], ['وعبارة'], ['الكنز'], ['ولو'], ['لم'], ['يقل'], ['من'], ['مالي'], ['ولا'], ['من'], ['غنمي'], ['لم'], ['يتعين'], ['غنمه'], ['إن'], ['كانت'], ['انتهت'], ['ا'], ['ه'], ['سم'], ['قوله'], ['فيعطى'], ['واحدة'], ['منها'], ['إلخ'], ['كما'], ['لو'], ['كانت'], ['موجودة'], ['عند'], ['الوصية'], ['والموت'], [], ['ولا'], ['يجوز'], ['أن'], ['يعطى'], ['واحدة'], ['من'], ['غير'], ['غنمه'], ['في'], ['الصورتين'], ['وإن'], ['تراضيا'], [], ['لأنه'], ['صلح'], ['على'], ['مجهول'], ['مغني'], ['ونهاية'], ['قال'], ['ع'], ['ش'], ['قوله'], ['واحدة'], ['منها'], ['أي'], ['كاملة'], [], ['ولا'], ['يجوز'], ['أن'], ['يعطى'], ['نصفين'], ['من'], ['شاتين'], [], ['لأنه'], ['لا'], ['يسمى'], ['شاة'], ['وقوله'], ['ولا'], ['يجوز'], ['أن'], ['يعطى'], ['واحدة'], ['من'], ['غير'], ['غنمه'], ['وينبغي'], ['أن'], ['يقال'], ['مثل'], ['ذلك'], ['في'], ['الأرقاء'], ['ا'], ['ه'], []]\n",
            "dict of diacritics =  [[{1: 'َ'}, {3: 'ْ'}, {5: 'ُ'}, {7: 'ُ'}], [{1: 'ِ'}, {3: 'َ'}, {5: 'َ'}, {7: 'ِ'}], [{1: 'َ'}], [{1: 'َ'}, {3: 'َ'}, {5: 'َ'}, {7: 'ّ'}, {8: 'َ'}, {10: 'ُ'}], [{2: 'َ'}, {4: 'ْ'}], [{1: 'َ'}, {3: 'ْ'}], [{2: 'ْ'}, {4: 'َ'}, {6: 'ِ'}, {8: 'ّ'}, {9: 'َ'}, {11: 'ُ'}], [{1: 'َ'}, {3: 'ْ'}, {5: 'ُ'}, {7: 'ُ'}], [{1: 'َ'}], [{1: 'َ'}, {3: 'ّ'}, {4: 'َ'}], [{1: 'َ'}, {3: 'ْ'}], [{1: 'ُ'}, {3: 'َ'}, {5: 'ْ'}, {7: 'َ'}], [{1: 'َ'}, {3: 'ْ'}, {5: 'ِ'}], [{2: 'ْ'}, {4: 'َ'}, {6: 'ْ'}, {8: 'ِ'}], [{1: 'َ'}, {3: 'َ'}, {5: 'ْ'}], [{1: 'َ'}, {3: 'َ'}, {5: 'ْ'}], [{2: 'ْ'}, {4: 'َ'}, {6: 'َ'}, {8: 'َ'}], [{1: 'َ'}, {3: 'َ'}], [{1: 'َ'}, {3: 'ْ'}, {5: 'َ'}, {7: 'ْ'}], [{1: 'َ'}, {3: 'ُ'}], [{1: 'ِ'}, {3: 'َ'}, {6: 'ٍ'}], [{1: 'َ'}, {3: 'ْ'}], [{1: 'َ'}, {3: 'ْ'}, {5: 'ُ'}, {8: 'ُ'}], [{1: 'َ'}, {4: 'ً'}], [{1: 'َ'}, {3: 'َ'}], [{1: 'َ'}, {3: 'َ'}, {5: 'َ'}], [{1: 'َ'}, {3: 'ُ'}], [{1: 'ِ'}, {3: 'ْ'}, {5: 'َ'}], [{2: 'ْ'}, {4: 'َ'}, {6: 'ْ'}, {8: 'ِ'}], [{1: 'َ'}, {3: 'ْ'}], [{1: 'َ'}, {3: 'ْ'}, {5: 'ُ'}, {7: 'ُ'}], [{2: 'ْ'}, {4: 'َ'}, {6: 'ِ'}, {8: 'ّ'}, {9: 'َ'}, {11: 'ُ'}], [{1: 'َ'}, {3: 'ْ'}], [{1: 'ُ'}, {3: 'ْ'}, {5: 'َ'}, {7: 'َ'}], [{1: 'َ'}, {3: 'ُ'}], [{1: 'َ'}, {4: 'ٌ'}], [{1: 'َ'}, {3: 'ُ'}, {5: 'ْ'}, {7: 'َ'}, {9: 'ُ'}], [{1: 'ِ'}, {3: 'ْ'}], [{1: 'َ'}, {3: 'ْ'}, {5: 'ِ'}, {7: 'ِ'}], [{2: 'ْ'}, {5: 'ِ'}], [{1: 'َ'}, {3: 'َ'}], [{1: 'َ'}, {3: 'ْ'}], [{1: 'َ'}, {3: 'ْ'}], [{1: 'َ'}, {3: 'ُ'}, {5: 'ْ'}], [{1: 'ِ'}, {3: 'ْ'}], [{1: 'َ'}, {4: 'ِ'}], [{1: 'َ'}, {3: 'َ'}], [{1: 'ِ'}, {3: 'ْ'}], [{1: 'َ'}, {3: 'َ'}, {5: 'ِ'}], [{1: 'َ'}, {3: 'ّ'}, {4: 'َ'}, {6: 'َ'}], [{1: 'َ'}], [{1: 'َ'}, {3: 'ْ'}, {5: 'ُ'}, {7: 'ُ'}], [], [{1: 'َ'}, {3: 'ِ'}, {5: 'َ'}, {8: 'َ'}, {10: 'ُ'}], [{2: 'ْ'}, {4: 'َ'}, {6: 'ْ'}, {8: 'ِ'}], [{1: 'َ'}, {3: 'َ'}, {5: 'ْ'}], [{1: 'َ'}, {3: 'ْ'}], [{1: 'َ'}, {3: 'ُ'}, {5: 'ْ'}], [{1: 'ِ'}, {3: 'ْ'}], [{1: 'َ'}, {4: 'ِ'}], [{1: 'َ'}, {3: 'َ'}], [{1: 'ِ'}, {3: 'ْ'}], [{1: 'َ'}, {3: 'َ'}, {5: 'ِ'}], [{1: 'َ'}, {3: 'ْ'}], [{1: 'َ'}, {3: 'َ'}, {5: 'َ'}, {7: 'ّ'}, {8: 'َ'}, {10: 'ْ'}], [{1: 'َ'}, {3: 'َ'}, {5: 'ُ'}, {7: 'ُ'}], [{2: 'ْ'}], [{1: 'َ'}, {4: 'َ'}, {6: 'ْ'}], [{2: 'ْ'}, {4: 'َ'}, {6: 'َ'}, {8: 'ْ'}], [], [], [], [{1: 'َ'}, {3: 'ْ'}, {5: 'ُ'}, {7: 'ُ'}], [{1: 'َ'}, {3: 'ُ'}, {5: 'ْ'}, {7: 'َ'}], [{1: 'َ'}, {4: 'ِ'}, {6: 'َ'}, {8: 'ً'}], [{1: 'ِ'}, {3: 'ْ'}, {5: 'َ'}], [{2: 'َ'}, {4: 'ْ'}], [{1: 'َ'}, {3: 'َ'}], [{1: 'َ'}, {3: 'ْ'}], [{1: 'َ'}, {4: 'َ'}, {6: 'ْ'}], [{1: 'َ'}, {3: 'ْ'}, {5: 'ُ'}, {8: 'َ'}, {10: 'ً'}], [{1: 'ِ'}, {3: 'ْ'}, {5: 'َ'}], [{2: 'ْ'}, {4: 'َ'}, {6: 'ِ'}, {8: 'ّ'}, {9: 'َ'}, {11: 'ِ'}], [{1: 'َ'}, {4: 'ْ'}, {6: 'َ'}, {8: 'ْ'}, {10: 'ِ'}], [], [{1: 'َ'}, {3: 'َ'}], [{1: 'َ'}, {3: 'ُ'}, {6: 'ُ'}], [{1: 'َ'}, {3: 'ْ'}], [{1: 'ُ'}, {3: 'ْ'}, {5: 'َ'}], [{1: 'َ'}, {4: 'ِ'}, {6: 'َ'}, {8: 'ً'}], [{1: 'ِ'}, {3: 'ْ'}], [{1: 'َ'}, {3: 'ْ'}, {5: 'ِ'}], [{1: 'َ'}, {3: 'َ'}, {5: 'ِ'}, {7: 'ِ'}], [{1: 'ِ'}], [{3: 'ّ'}, {4: 'ُ'}, {7: 'َ'}, {9: 'َ'}, {11: 'ْ'}, {13: 'ِ'}], [{1: 'َ'}, {3: 'ِ'}, {5: 'ْ'}], [{1: 'َ'}, {3: 'َ'}, {6: 'َ'}, {8: 'َ'}], [], [{1: 'ِ'}, {3: 'َ'}, {5: 'ّ'}, {6: 'َ'}, {8: 'ُ'}], [{1: 'ُ'}, {3: 'ْ'}, {5: 'ٌ'}], [{1: 'َ'}, {3: 'َ'}], [{1: 'َ'}, {3: 'ْ'}, {5: 'ُ'}, {8: 'ٍ'}], [{1: 'ُ'}, {3: 'ْ'}, {5: 'ِ'}], [{1: 'َ'}, {3: 'ِ'}, {5: 'َ'}, {8: 'َ'}, {10: 'ٌ'}], [{1: 'َ'}, {4: 'َ'}], [], [], [{1: 'َ'}, {3: 'ْ'}, {5: 'ُ'}, {7: 'ُ'}], [{1: 'َ'}, {4: 'ِ'}, {6: 'َ'}, {8: 'ً'}], [{1: 'ِ'}, {3: 'ْ'}, {5: 'َ'}], [{1: 'َ'}, {3: 'ْ'}], [{1: 'َ'}, {4: 'ِ'}, {6: 'َ'}, {8: 'ً'}], [], [{1: 'َ'}, {3: 'َ'}], [{1: 'َ'}, {3: 'ُ'}, {6: 'ُ'}], [{1: 'َ'}, {3: 'ْ'}], [{1: 'ُ'}, {3: 'ْ'}, {5: 'َ'}], [{1: 'ِ'}, {3: 'ْ'}, {5: 'َ'}, {7: 'ْ'}, {9: 'ِ'}], [{1: 'ِ'}, {3: 'ْ'}], [{1: 'َ'}, {4: 'َ'}, {6: 'ْ'}, {8: 'ِ'}], [], [{1: 'ِ'}, {3: 'َ'}, {5: 'ّ'}, {6: 'َ'}, {8: 'ُ'}], [{1: 'َ'}], [{1: 'ُ'}, {3: 'َ'}, {5: 'ّ'}, {6: 'َ'}], [{1: 'َ'}, {4: 'ً'}], [{1: 'َ'}, {3: 'َ'}, {5: 'ْ'}, {7: 'ُ'}, {9: 'ُ'}], [{1: 'َ'}, {3: 'َ'}], [{1: 'َ'}, {3: 'ُ'}, {6: 'ُ'}], [{1: 'َ'}, {3: 'ْ'}], [{1: 'ُ'}, {3: 'ْ'}, {5: 'َ'}], [{1: 'َ'}, {4: 'ِ'}, {6: 'َ'}, {8: 'ً'}], [{1: 'ِ'}, {3: 'ْ'}], [{1: 'َ'}, {3: 'ْ'}, {5: 'ِ'}], [{1: 'َ'}, {3: 'َ'}, {5: 'ِ'}, {7: 'ِ'}], [{1: 'َ'}, {3: 'َ'}, {5: 'ْ'}, {7: 'َ'}, {9: 'ِ'}], [{1: 'َ'}, {3: 'ْ'}], [{1: 'ُ'}, {3: 'َ'}, {6: 'َ'}], [{1: 'ِ'}, {3: 'ْ'}, {5: 'ُ'}], [{1: 'َ'}, {3: 'ِ'}, {5: 'َ'}], [{1: 'ِ'}], [{2: 'ْ'}, {4: 'َ'}, {6: 'ِ'}, {8: 'ّ'}, {9: 'َ'}, {12: 'ِ'}], [], [], []]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Extraction"
      ],
      "metadata": {
        "id": "Ya6ekNo0UfYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arabic_alphabet = [\n",
        "    \"أ\", \"ب\", \"ت\", \"ث\", \"ج\", \"ح\", \"خ\", \"د\", \"ذ\", \"ر\", \"ز\", \"س\", \"ش\", \"ص\",\n",
        "    \"ض\", \"ط\", \"ظ\", \"ع\", \"غ\", \"ف\", \"ق\", \"ك\", \"ل\", \"م\", \"ن\", \"ه\", \"و\", \"ي\", \"آ\", \"ؤ\", \"ئ\", \"ى\", \"ء\", \"ا\"\n",
        "]\n",
        "encoder = OneHotEncoder(sparse=False, categories=[arabic_alphabet])\n",
        "one_hot_encoded = encoder.fit_transform([[category] for category in arabic_alphabet])\n",
        "\n",
        "print(one_hot_encoded.shape)"
      ],
      "metadata": {
        "id": "E0tCzKDOBux3",
        "outputId": "72cd2bd4-278c-49bc-e203-6e3cf8fec023",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(34, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "# sentences = [train_dataset[0]] + [train_dataset[1]]\n",
        "sentences_diactrics_dict = []\n",
        "for text in train_dataset:\n",
        "    sentence = remove_non_arabic_char(text)\n",
        "    sentence, d = remove_diacritics_and_save(sentence)\n",
        "    segmented = segmenter(sentence)\n",
        "    sentences_diactrics_dict.append(d)\n",
        "    for s in segmented:\n",
        "        tokens = word_tokenize(s)\n",
        "        sentences.append(tokens)\n",
        "    # print(len(sentence))\n",
        "    # print(segmented)\n"
      ],
      "metadata": {
        "id": "ymEi5AEKM-Ij"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[0])"
      ],
      "metadata": {
        "id": "WGhKbCOdTW2U",
        "outputId": "5f86b6ff-3dbf-45c5-a0fe-54c3e0535111",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "longest_length = max(len(lst) for lst in sentences)\n",
        "print(longest_length)\n",
        "index_of_longest_list = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "print(index_of_longest_list)\n",
        "print(sentences[index_of_longest_list])"
      ],
      "metadata": {
        "id": "LC4qrsluUifT",
        "outputId": "d05676a1-2721-4569-96fa-ab568edad22c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "307\n",
            "139034\n",
            "['ش', 'قوله', 'إن', 'عمر', 'بن', 'الخطاب', 'استشار', 'في', 'الخمر', 'يشربها', 'الرجل', 'وجواب', 'علي', 'يدل', 'على', 'أنه', 'إنما', 'استشار', 'في', 'قدر', 'الحد', 'وإنما', 'كان', 'ذلك', 'لأن', 'الأصح', 'أنه', 'لم', 'يتقرر', 'في', 'زمن', 'النبي', 'صلى', 'الله', 'عليه', 'وسلم', 'بمعنى', 'أنه', 'لم', 'يحد', 'فيه', 'حدا', 'بقول', 'يعلم', 'لا', 'يزاد', 'عليه', 'ولا', 'ينقص', 'عنه', 'وإنما', 'كان', 'يضرب', 'مقدارا', 'قدرته', 'الصحابة', 'واختلفوا', 'في', 'تقديره', 'يدل', 'على', 'ذلك', 'ما', 'روي', 'عن', 'علي', 'بن', 'أبي', 'طالب', 'رضي', 'الله', 'عنه', 'أنه', 'قال', 'ما', 'من', 'رجل', 'أقمت', 'عليه', 'حدا', 'فمات', 'فأجد', 'في', 'نفسي', 'منه', 'شيئا', 'إلا', 'شارب', 'الخمر', 'فإنه', 'إن', 'مات', 'فيه', 'وديته', 'لأن', 'رسول', 'الله', 'صلى', 'الله', 'عليه', 'وسلم', 'لم', 'يبينه', 'ومعنى', 'ذلك', 'أنه', 'لم', 'يحده', 'بقول', 'يحصره', 'ويمنع', 'الزيادة', 'فيه', 'والنقص', 'منه', 'فحدوه', 'باجتهادهم', 'وروى', 'أنس', 'أتي', 'النبي', 'صلى', 'الله', 'عليه', 'وسلم', 'برجل', 'قد', 'شرب', 'الخمر', 'فجلده', 'بجريدتين', 'نحوا', 'من', 'أربعين', 'وفعله', 'أبو', 'بكر', 'فلما', 'كان', 'عمر', 'استشار', 'الناس', 'فقال', 'عبد', 'الرحمن', 'أخف', 'الحدود', 'ثمانون', 'فأمر', 'به', 'عمر', 'وقد', 'تقدم', 'من', 'قول', 'علي', 'بن', 'أبي', 'طالب', 'أنه', 'قال', 'إذا', 'شرب', 'سكر', 'وإذا', 'سكر', 'هذى', 'وإذا', 'هذى', 'افترى', 'فقاسه', 'على', 'المفتري', 'واستدل', 'أن', 'ذلك', 'حكمه', 'وإلى', 'هذا', 'ذهب', 'مالك', 'وأبو', 'حنيفة', 'أن', 'حد', 'شارب', 'الخمر', 'ثمانون', 'وقال', 'الشافعي', 'أربعون', 'والدليل', 'على', 'أن', 'ما', 'نقوله', 'ما', 'روي', 'من', 'الأحاديث', 'الدالة', 'على', 'أنه', 'لم', 'يكن', 'من', 'النبي', 'صلى', 'الله', 'عليه', 'وسلم', 'نص', 'في', 'ذلك', 'على', 'تحديد', 'وكان', 'الناس', 'على', 'ذلك', 'ثم', 'وقع', 'الاجتهاد', 'في', 'ذلك', 'في', 'زمن', 'عمر', 'بن', 'الخطاب', 'ولم', 'يوجد', 'عند', 'أحد', 'منهم', 'نص', 'على', 'تحديد', 'وذلك', 'من', 'أقوى', 'الدليل', 'على', 'عدم', 'النص', 'فيه', 'لأنه', 'لا', 'يصح', 'أن', 'يكون', 'فيه', 'نص', 'باق', 'حكمه', 'ويذهب', 'على', 'الأمة', 'لأن', 'ذلك', 'كأن', 'يكون', 'إجماعا', 'منهم', 'على', 'الخطأ', 'ولا', 'يجوز', 'ذلك', 'على', 'الأمة', 'ثم', 'أجمعوا', 'واتفقوا', 'أن', 'الحد', 'ثمانون', 'وحكم', 'بذلك', 'على', 'ملأ', 'منهم', 'ولم', 'يعلم', 'لأحد', 'فيه', 'مخالفة', 'فثبت', 'أنه', 'إجماع', 'ودليلنا', 'من', 'جهة', 'القياس', 'أن', 'هذا', 'حد', 'في', 'معصية', 'فلم', 'يكن', 'أقل', 'من', 'ثمانين', 'كحد', 'الفرية', 'والزنى']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Word2Vec model (using Skip-gram, you can also use CBOW)\n",
        "model = Word2Vec(sentences=sentences, vector_size=300, window=20, sg=1, min_count=1)\n",
        "model.train(sentences, total_examples=len(sentences), epochs=10)\n",
        "# Save the model (optional)\n",
        "model.save(\"word2vec_model\")\n",
        "\n",
        "# Load the model (optional)\n",
        "# model = Word2Vec.load(\"word2vec_model\")\n",
        "\n",
        "# Get the vector representation of a word\n",
        "word_vector = model.wv[\"قال\"]\n",
        "\n",
        "word = \"أو\"\n",
        "# Find similar words\n",
        "similar_words = model.wv.most_similar(word, topn=5)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Vector representation of {word}:\", word_vector)\n",
        "print(f\"Similar words to {word}:\", similar_words)"
      ],
      "metadata": {
        "id": "pB5CqSP29yqj",
        "outputId": "45fb073a-6b15-4d6c-de4e-040a04229aba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector representation of أو: [-6.99151084e-02  3.05549949e-01 -1.71290725e-01 -1.68057159e-02\n",
            "  1.93623349e-01 -2.39820838e-01  2.10489348e-01  4.52250868e-01\n",
            "  1.24369346e-01 -6.09374642e-02 -2.86353249e-02 -4.67340433e-04\n",
            "  2.16338217e-01  3.36695672e-03 -5.73583357e-02 -9.78622679e-03\n",
            "  1.45897582e-01  2.13593945e-01 -2.97793075e-02  1.07618831e-01\n",
            " -2.45435476e-01  3.40700485e-02 -4.71212715e-02 -1.23616494e-01\n",
            "  1.47061974e-01 -1.41845271e-01 -2.65673310e-01  6.04879782e-02\n",
            " -1.70172029e-03 -2.09539101e-01 -2.46850178e-01 -3.65248993e-02\n",
            "  2.08183825e-01 -1.12353303e-01 -6.57649264e-02  1.42010152e-01\n",
            " -1.03371799e-01 -1.09847613e-01 -4.94952016e-02 -1.02198392e-01\n",
            " -7.38092586e-02 -1.11796044e-01 -1.54698536e-01  1.62741728e-02\n",
            " -3.30022983e-02  1.01303622e-01 -3.92164886e-02  1.62024185e-01\n",
            "  2.93581307e-01  1.92659333e-01  1.52386665e-01  2.95715183e-02\n",
            "  6.80400282e-02 -8.55665058e-02 -2.23495029e-02  2.51394585e-02\n",
            "  4.59075458e-02  3.02601587e-02 -9.24976449e-03 -2.09428534e-01\n",
            " -1.11281745e-01 -8.46995320e-03  2.93346886e-02 -9.06052366e-02\n",
            " -1.25766382e-01  1.36609077e-01 -4.66868542e-02  2.10029036e-02\n",
            "  7.84060955e-02 -2.26440132e-01  1.15046777e-01  2.57465482e-01\n",
            "  2.66261101e-02 -1.13645613e-01 -1.30823389e-01  1.00381173e-01\n",
            " -2.07727283e-01  1.68442696e-01  2.85837352e-01  7.18230307e-02\n",
            " -1.63997814e-01  2.69103795e-01  6.12367392e-02  9.33980271e-02\n",
            "  2.16308996e-01 -1.79356426e-01 -2.92016685e-01  1.35057392e-02\n",
            "  1.25477910e-01  1.35873944e-01 -6.24268688e-02 -1.46610737e-02\n",
            "  6.53692149e-03  1.44416809e-01  1.31254002e-01  1.22128129e-02\n",
            "  9.81750935e-02  2.04475626e-01 -1.86582401e-01 -2.34248191e-02\n",
            "  2.55978942e-01 -1.47551686e-01  1.72736079e-01  1.91919252e-01\n",
            " -2.53738821e-01  1.30964741e-01  7.56848305e-02 -1.96850556e-03\n",
            " -1.02033764e-02 -3.22148502e-02 -8.37791618e-03  5.44997938e-02\n",
            " -1.02034301e-01  1.97210446e-01 -2.35692069e-01  1.29487917e-01\n",
            "  5.08440398e-02 -4.51050662e-02  2.82763243e-01 -2.88977653e-01\n",
            " -6.53843582e-02  8.00403804e-02 -1.68293446e-01 -1.24006703e-01\n",
            " -6.34203181e-02 -2.49545500e-02  2.14724839e-02  7.88446218e-02\n",
            "  8.81707966e-02  2.11866319e-01 -7.36702383e-02 -2.65039742e-01\n",
            "  4.41192776e-01 -3.02989513e-01  1.39010951e-01  2.06951350e-02\n",
            " -2.38117635e-01 -7.22559616e-02  8.71390011e-03  1.77547950e-02\n",
            " -2.83236690e-02  1.72235351e-02  3.54089253e-02 -7.82824233e-02\n",
            " -3.59621644e-02  1.57451168e-01 -2.83811569e-01 -2.89901465e-01\n",
            " -1.32025465e-01 -6.33513108e-02  2.18690485e-01 -1.96743309e-02\n",
            " -2.72662878e-01 -1.00798480e-01  7.76142906e-03  8.86240602e-03\n",
            " -4.26667243e-01 -2.02188075e-01 -4.02208120e-02 -1.99820083e-02\n",
            "  5.03157862e-02  1.41857937e-01 -1.36313140e-01 -2.34881818e-01\n",
            " -2.95868576e-01  2.92402297e-01  2.49894038e-02  5.19287549e-02\n",
            "  5.43875732e-02 -1.63323283e-02 -1.12553708e-01  1.19764440e-01\n",
            " -7.03500137e-02  3.24450105e-01  1.24913953e-01 -1.66036844e-01\n",
            " -2.43801102e-01 -2.32454404e-01  9.63205323e-02 -1.12573504e-01\n",
            " -8.08622781e-03  2.24971265e-01 -1.68339238e-01  4.00954150e-02\n",
            " -6.06964603e-02 -2.86478400e-01  3.08759779e-01  3.89177322e-01\n",
            "  2.51849592e-01 -5.16979635e-01  8.61118734e-02 -1.13733456e-01\n",
            " -2.43341923e-01  1.45418718e-01  1.25910640e-01 -2.57972851e-02\n",
            "  6.57739788e-02  9.28961672e-03  1.39401346e-01 -2.68982083e-01\n",
            " -1.07519748e-02 -2.61531115e-01 -3.74799557e-02  2.97181103e-02\n",
            "  6.40875772e-02 -3.70939188e-02  6.69601932e-02  1.40592288e-02\n",
            "  6.55539334e-02  6.55872300e-02 -3.78358876e-03 -1.50894746e-01\n",
            " -1.16416216e-01  7.16938823e-02  3.29909325e-02 -2.78093487e-01\n",
            " -1.37841806e-01 -2.32029691e-01 -1.63544603e-02  8.36891830e-02\n",
            "  1.99951082e-01  2.21096855e-02  1.32514045e-01  1.01151221e-04\n",
            "  6.54067844e-02  2.12004229e-01  2.18359098e-01 -2.17191175e-01\n",
            " -1.30898103e-01  3.12220193e-02  5.95081300e-02 -1.37698829e-01\n",
            "  2.13082239e-01  1.57261953e-01  1.99592158e-01  3.79428834e-01\n",
            " -5.26207499e-02  6.41785190e-02  1.36953935e-01 -4.20603901e-01\n",
            " -1.90874070e-01  1.83387641e-02  8.05470496e-02  2.88333353e-02\n",
            "  1.59467816e-01 -6.84665591e-02 -1.96343362e-02  5.58148250e-02\n",
            " -1.45920038e-01  1.88358396e-01  1.56459510e-01  8.91688105e-04\n",
            "  3.33043300e-02  5.63768521e-02 -2.75442481e-01  1.31039143e-01\n",
            " -1.07243262e-01 -9.60976258e-02 -1.58265512e-02  3.69615704e-02\n",
            " -5.95308766e-02  1.87844615e-02  3.48112375e-01 -3.44154805e-01\n",
            " -8.37825909e-02 -1.32078618e-01  2.59034783e-01  4.52891439e-02\n",
            " -4.33881022e-02  2.12150425e-01  1.24279343e-01 -3.15296799e-02\n",
            " -2.68304367e-02 -1.21386945e-01  1.97497874e-01  2.18305841e-01\n",
            "  1.49481773e-01 -5.33338860e-02 -7.54655674e-02  7.77168348e-02\n",
            " -9.77382734e-02 -1.98190227e-01  2.24241130e-02  6.96120039e-02\n",
            " -1.15296751e-01 -9.42286570e-03 -2.29473114e-02 -1.91028014e-01\n",
            "  2.56599724e-01  8.56477469e-02 -1.58932731e-01  1.42160520e-01\n",
            " -5.52118272e-02  2.19640851e-01  3.27686705e-02  3.09284478e-01\n",
            " -2.23138202e-02  3.87533046e-02 -6.53006583e-02 -1.10945217e-01]\n",
            "Similar words to أو: [('معارا', 0.547890305519104), ('من', 0.5477692484855652), ('بتمجس', 0.5320965051651001), ('المعوضان', 0.5304346680641174), ('فجرا', 0.5265641212463379)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vector = model.wv[\"قال\"]\n",
        "\n",
        "word = \"جميل\"\n",
        "# Find similar words\n",
        "similar_words = model.wv.most_similar(word, topn=5)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Vector representation of {word}:\", word_vector)\n",
        "print(f\"Similar words to {word}:\", similar_words)"
      ],
      "metadata": {
        "id": "gNyU1SXvIr1S",
        "outputId": "fa0ba25f-11c1-4231-eb10-263805004e20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-171-e580ca755f43>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"قال\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"جميل\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Find similar words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msimilar_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'wv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Sample dataset\n",
        "arabic_text = [\"مرحبا\", \"اليوم\", \"في\", \"مدرسة\"]\n",
        "diacritics = [\"مَرْحَبًا\", \"الْيَوْمِ\", \"فِي\", \"مَدْرَسَةِ\"]\n",
        "\n",
        "# Tokenize characters and diacritics\n",
        "tokenizer_chars = Tokenizer(char_level=True)\n",
        "tokenizer_chars.fit_on_texts(arabic_text)\n",
        "\n",
        "tokenizer_diacritics = Tokenizer(char_level=True)\n",
        "tokenizer_diacritics.fit_on_texts(diacritics)\n",
        "\n",
        "# Convert characters to sequences\n",
        "X_sequences = tokenizer_chars.texts_to_sequences(arabic_text)\n",
        "y_sequences = tokenizer_diacritics.texts_to_sequences(diacritics)\n",
        "\n",
        "# Padding sequences to have the same length\n",
        "X_padded = pad_sequences(X_sequences, padding='post')\n",
        "y_padded = pad_sequences(y_sequences, padding='post')\n",
        "\n",
        "# Define the model\n",
        "model_RNN = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer_chars.word_index) + 1, output_dim=32, input_length=X_padded.shape[1]),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    TimeDistributed(Dense(len(tokenizer_diacritics.word_index) + 1, activation='softmax'))\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_RNN.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_RNN.fit(X_padded, y_padded, epochs=10, batch_size=1, validation_split=0.2)\n",
        "\n",
        "# Example predictions\n",
        "sample_text = [\"السلام\"]\n",
        "sample_sequence = tokenizer_chars.texts_to_sequences(sample_text)\n",
        "sample_padded = pad_sequences(sample_sequence, padding='post', maxlen=X_padded.shape[1])\n",
        "predictions = model_RNN.predict(sample_padded)\n",
        "\n",
        "# Decode predictions\n",
        "predicted_diacritics = tokenizer_diacritics.sequences_to_texts(np.argmax(predictions, axis=-1))\n",
        "\n",
        "print(f\"Input Text: {sample_text[0]}\")\n",
        "print(f\"Predicted Diacritics: {predicted_diacritics[0]}\")"
      ],
      "metadata": {
        "id": "yx7ivnWaPjfO",
        "outputId": "83de2bed-2d49-4f60-a4ba-8dd2f816ac52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-170-33c31382db40>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Example predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1127, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n        return backend.sparse_categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5777, in sparse_categorical_crossentropy\n        res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\n    ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(1, 10) and logits.shape=(1, 5, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9o-eudejm6LC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}